<!DOCTYPE html>
<html>
<head>
<script type="text/javascript" async src="https://cdn.jsdelivr.net/gh/mathjax/MathJax@2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<link rel="Stylesheet" type="text/css" href="style.css">
<title>orthogonality-least-squares</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>

<div id="Orthogonality &amp; least squares"><h2 id="Orthogonality &amp; least squares">Orthogonality &amp; least squares</h2></div>
<p>
let \(u,v \in \Re^n\). orthogonal iff:
</p>
<ul>
<li>
\(u \cdot v = 0\)

<li>
or \(\|u\|^2 + \|v\|^2 = \|u+v\|^2\)

</ul>

<div id="Orthogonality &amp; least squares-Inner (dot) product &amp; uses"><h3 id="Inner (dot) product &amp; uses">Inner (dot) product &amp; uses</h3></div>
<p>
let \(u,v \in \Re^n\). then, \(u \cdot v = u^T v \in \Re\).
</p>

<p>
in English, to calculate you just multiply the vectors row-wise, and sum all the results.
</p>

<p>
Regular algebraic rules apply.
</p>

<p>
\(u \cdot u \geq 0\), only 0 iff u = 0.
</p>

<div id="Orthogonality &amp; least squares-Inner (dot) product &amp; uses-Length of a vector"><h4 id="Length of a vector">Length of a vector</h4></div>
<p>
Let \(v \in \Re^n\), then the norm (length) of v is \(\|v\| = \sqrt{v \cdot v}\).
</p>

<p>
Does the norm coincide with length of line segments? Yes:
</p>

<p>
\(x = \begin{bmatrix}a\\b\end{bmatrix}, \quad \|v\| = \sqrt{v \cdot v} = \sqrt{a^2 + b^2} = \text{Pythagoras}\)
</p>

<div id="Orthogonality &amp; least squares-Inner (dot) product &amp; uses-Distance between vectors"><h4 id="Distance between vectors">Distance between vectors</h4></div>
<p>
Let \(u,v \in \Re^n\). then, \(\text{dist}(u,v) = \|u-v\|\).
</p>

<div id="Orthogonality &amp; least squares-Orthogonal complement"><h3 id="Orthogonal complement">Orthogonal complement</h3></div>
<p>
Let \(W \subset \Re^n\) a subspace, then orthogonal complement of W is \(W^\perp = \{x \in \Re^n | x \cdot v = 0 \forall u \in W \}\)
</p>

<p>
properties:
</p>
<ul>
<li>
\((colA)^\perp = (NulA)^T\)

<li>
\((NulA)^\perp = (colA)^T\)

</ul>

<div id="Orthogonality &amp; least squares-Orthogonal sets"><h3 id="Orthogonal sets">Orthogonal sets</h3></div>
<p>
a set \(\{v_1 \dots v_p\}\) is orthogonal if \(v_i \cdot v_j = 0 \forall i,j\). then \(\{v_1 \dots v_p\}\) is a basis for \(\text{Span}\{v_1 \dots v_p\}\)
</p>

<p>
An orthogonal basis is a basis that is also an orthogonal set
</p>

<p>
Why orthogonal basis? Let \(W \in \Re^n\) be subspace with orthogonal basis \(\{u_1 \dots u_p\}\), then \(W \ni y = c_1 u_1 + \ldots + c_p u_p\), with \(c_i = \frac{y \cdot u_i}{u_i \cdot u_i}\) for i = 1...p.
</p>

<p>
An orthonormal set/basis is an orthogonal set/basis consisting of unit vectors (like \(\{e_1, \ldots, e_n\}\text{ for }\Re^n\)).
</p>

<p>
An m × matrix A has orthonormal columns iff \(A^T A = I_n\)
</p>
<ul>
<li>
\((Ax) \cdot (Ay) = x \cdot y\)

<li>
\(\| Ax \| = \| x \|\)

</ul>

<div id="Orthogonality &amp; least squares-Orthogonal projections"><h3 id="Orthogonal projections">Orthogonal projections</h3></div>
<div id="Orthogonality &amp; least squares-Orthogonal projections-Orthogonal decomposition"><h4 id="Orthogonal decomposition">Orthogonal decomposition</h4></div>
<p>
Let W be a subspace of \(\Re^n\). Each y in \(R^n\) can be written uniquely in \(y = \hat{y}+z\) (\(\hat{y} \in W,\; z \in W^\perp\))
</p>

<p>
If \(\{u_1, \ldots, u_p\}\) in orthogonal basis of W, then \(\hat{y} = \frac{y \cdot u_1}{u_1 \cdot u_1} u_1 + \ldots + \frac{y \cdot u_p}{u_p \cdot u_p}u_p\)
</p>

<p>
ŷ is an orthogonal projection of y onto W (\(proj_w y\))
</p>

<div id="Orthogonality &amp; least squares-Orthogonal projections-Best approximation"><h4 id="Best approximation">Best approximation</h4></div>
<p>
Let W be subspace of \(\Re^n\), y a vector in \(\Re^n\), ŷ an orthogonal projection of y onto W.
</p>

<p>
Then \(\|y-\hat{y}\| &lt; \|y-v\|\)
</p>

<div id="Orthogonality &amp; least squares-Orthogonal projections-When basis for W is an orthonormal set"><h4 id="When basis for W is an orthonormal set">When basis for W is an orthonormal set</h4></div>
<p>
If \(\{u_1 \ldots u_p\}\) is orthonormal basis for subspace W of \(\Re^n\), then \(\text{proj}_w y = (y \cdot u_1)u_1 + \dots + (y \cdot u_p) u_p\)
</p>

<p>
If U = \(\begin{bmatrix} u_1 &amp; u_2 &amp; \dots &amp; u_p \end{bmatrix}\), then \(\text{proj}_w y = UU^T y \quad \forall y \in \Re^n\)
</p>

<div id="Orthogonality &amp; least squares-Gram-Schmidt process"><h3 id="Gram-Schmidt process">Gram-Schmidt process</h3></div>
<p>
An algorithm for producing orthogonal or orthonormal basis for any nonzero subspace of \(\Re^n\).
</p>

<p>
Given basis \(\{ x_1 \dots x_p \}\) for nonzero subspace W of \(\Re^n\), define:
</p>

<p>
{{\(%align*\)
v_1 &amp;= x_1\\
v_2 &amp;= x_2 - \frac{x_2 \cdot v_1}{v_1 \cdot v_1} v_1\\
v_3 &amp;= x_3 - \frac{x_3 \cdot v_1}{v_1 \cdot v_1} v_1 - \frac{x_3 \cdot v_2}{v_2 \cdot v_2} v_2\\
\vdots \\
v_p &amp;= x_p - \frac{x_p \cdot v_1}{v_1 \cdot v_1} v_1 - \dots - \frac{x_p \cdot v_{p-1}}{v_{p-1} \cdot v_{p-1} v_{p-1}}
}}$
</p>

<p>
Then \(\{v_1 \dots v_p\}\) is an orthogonal basis for W.
</p>

<p>
\(\text{Span}\{v_1 \dots v_k\} = \text{Span}\{x_1 \dots x+k\}\) for 1 ≤ k ≤ p.
</p>

<div id="Orthogonality &amp; least squares-Gram-Schmidt process-QR factorization"><h4 id="QR factorization">QR factorization</h4></div>
<p>
If A is an m × n matrix, with linearly independent columns, then A can be factored as \(A = QR\), where Q is he m×n matrix whose columns form an orthonormal basis for Col A, and R is n×n upper triangular invertible matrix with diagonal positive entries.
</p>

<div id="Orthogonality &amp; least squares-Least-squares problems"><h3 id="Least-squares problems">Least-squares problems</h3></div>
<p>
If a solution for \(Ax = b\) does not exist and one is needed, try to find the best approximation x for \(Ax = b\).
</p>

<p>
General least-squares problem is to find x that makes \(\| b - Ax\|\) as small as possible.
</p>

<p>
If A is m×n and \(b \in \Re^m\), a least-squares solution of \(Ax = b\) is \(\hat{x} \in \Re^n\) such that \(\| b - A\hat{x} \| \leq \| b - Ax \|, \qquad \forall x \in \Re^n\).
</p>

<p>
Least-square solution set of \(Ax = b\) is the same as the solution set for \(A^T Ax = A^T b\).
</p>

<p>
Therefore, \(\hat{x} = (A^T A)^{-1} A^T b\).
</p>

<p>
Given an m×n matrix A with linearly independent columns, let \(A = QR\) be a QR factorization of A. Then, for each \(b \in \Re^m\), \(Ax = b\) has unique least-squares solution:
</p>

<p>
\(\hat{x} = R^{-1} Q^T b\)
</p>

</body>
</html>
